{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leandro-driguez/Machine-Learning-Techniques/blob/dev/Taller_2/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SU_lch1DySrJ"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hectormelo/Machine-Learning-Techniques/main/Banner.png\" ><br>\n",
        "# Machine Learning Techniques - MISIS4219\n",
        "\n",
        "Primer semestre - 2024\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5UbZ1aWajfMi"
      },
      "source": [
        "\n",
        "# **Taller 2**\n",
        "\n",
        "GeoAlpes, una empresa líder en análisis geoespacial, está buscando mejorar sus técnicas de clasificación automática de imágenes satelitales. El objetivo es poder categorizar distintas características geográficas (como bosques, zonas industriales, zonas de cultivos permanentes, zonas residenciales y rios) con alta precisión y rapidez. Para lograrlo, están interesados en explorar las capacidades de los métodos ensemble.\n",
        "\n",
        "1.  Exploración y Preparación de Datos\n",
        "\n",
        "    -   Discuta las particularidades de las imágenes satelitales y sugiera técnicas extra de preprocesamiento de ser necesario.\n",
        "\n",
        "2.  Implementación de Gradient Boosting\n",
        "\n",
        "    -   Utilice Gradient Boosting como modelo base. Discuta las ventajas y desafíos de este método, y cómo afecta la precisión y robustez del clasificador. Además, compare el desempeño de Gradient Boosting con el modelo Random Forest presentado en la práctica, ¿logra observar mejoras significativas?\n",
        "\n",
        "3. Implementación de un nuevo metodo Ensemble\n",
        "    - Elija y presente un método ensemble de su preferencia.Introduzca y discuta el concepto del método elegido y cómo podría ser benéfico para la clasificación de imágenes satelitales.\n",
        "    - Compare el desempeño de su método elegido con Gradient Boosting y el modelo Random Forest. Discuta las ventajas y desventajas de cada uno.\n",
        "\n",
        "4.  Optimización y Ajuste\n",
        "\n",
        "    -   Realice una búsqueda de los mejores hiperparámetros para mejorar el desempeño de cada uno de los modelos implementados (Grid Search).\n",
        "\n",
        "Datos: [Enlace al sub conjunto del dataset de imágenes satelitales EuroSAT](https://github.com/hectormelo/Machine-Learning-Techniques/raw/main/Lab_2/EuroSAT3.zip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import stats\n",
        "from numpy import fliplr\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.color import label2rgb\n",
        "from skimage.transform import rotate\n",
        "from skimage.segmentation import slic\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pywt\n",
        "import mahotas\n",
        "import mahotas.features\n",
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_images(folder, img_size=(64, 64), use_slic_segmentation=False, n_segments=30, compactness=10):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        image_path = os.path.join(folder, filename)\n",
        "        image = Image.open(image_path)\n",
        "        image = np.array(image.convert(\"RGB\"))\n",
        "        \n",
        "        # normalization\n",
        "        normalized_image = image / 255.0\n",
        "        \n",
        "        # data augmentation\n",
        "        rotated_image = rotate(normalized_image, angle=90)\n",
        "        flipped_image = fliplr(normalized_image)\n",
        "        \n",
        "        # apply SLIC\n",
        "        if use_slic_segmentation:\n",
        "            segments = slic(normalized_image, n_segments=n_segments, compactness=compactness, start_label=1)\n",
        "            segmented_image = label2rgb(segments, normalized_image, kind='avg', bg_label=0)\n",
        "            images.append(segmented_image)\n",
        "        else:\n",
        "            images.extend([normalized_image, rotated_image, flipped_image])\n",
        "    \n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categories = ['Forest', 'Industrial', 'PermanentCrop', 'Residential', 'River']\n",
        "base_path = './EuroSAT3/'\n",
        "\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for label, category in enumerate(categories):\n",
        "    folder = os.path.join(base_path, category)\n",
        "    category_images = load_and_preprocess_images(folder)\n",
        "    all_images.extend(category_images)\n",
        "    all_labels.extend([label] * len(category_images))\n",
        "\n",
        "# Convertir listas a arrays de NumPy\n",
        "all_images_np = np.array(all_images)\n",
        "all_labels_np = np.array(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(all_images_np, all_labels_np, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_color_stats(image):\n",
        "    features = []\n",
        "\n",
        "    for channel in range(image.shape[-1]):\n",
        "        channel_data = image[:, :, channel].ravel()\n",
        "        \n",
        "        mean = np.mean(channel_data)\n",
        "        std = np.std(channel_data)\n",
        "        var = np.var(channel_data)\n",
        "        median = np.median(channel_data)\n",
        "        \n",
        "        mode_result = stats.mode(channel_data)\n",
        "        mode = mode_result.mode\n",
        "        \n",
        "        min_val = np.min(channel_data)\n",
        "        max_val = np.max(channel_data)\n",
        "        percentile_25 = np.percentile(channel_data, 25)\n",
        "        percentile_50 = np.percentile(channel_data, 50)\n",
        "        percentile_75 = np.percentile(channel_data, 75)\n",
        "        skewness = stats.skew(channel_data)\n",
        "        kurtosis = stats.kurtosis(channel_data)\n",
        "        data_range = max_val - min_val\n",
        "        \n",
        "        features.extend([\n",
        "            mean, std, var, median, mode, min_val, max_val,\n",
        "            percentile_25, percentile_50, percentile_75,\n",
        "            skewness, kurtosis, data_range\n",
        "        ])\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_color_histogram(image, bins=32, channel_range=(0, 256)):\n",
        "    histogram = [np.histogram(image[:, :, i], bins=bins, range=channel_range)[0] for i in range(image.shape[-1])]\n",
        "    return np.concatenate(histogram)\n",
        "\n",
        "\n",
        "def extract_haralick_features(image):\n",
        "    gray_image = rgb2gray(image)\n",
        "    gray_image = (gray_image * 255).astype('uint8')\n",
        "\n",
        "    features = mahotas.features.haralick(gray_image).mean(axis=0)\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_lbp_features(image, P=8, R=1, method='uniform'):\n",
        "\n",
        "    image_gray = rgb2gray(image)\n",
        "    lbp = local_binary_pattern(image_gray, P=P, R=R, method=method)\n",
        "\n",
        "    lbp_hist, _ = np.histogram(lbp, density=True, bins=np.arange(0, P + 3), range=(0, P + 2))\n",
        "    return lbp_hist\n",
        "\n",
        "\n",
        "def extract_fft_features(image):\n",
        "\n",
        "    fft_coefs = np.fft.fft2(rgb2gray(image))\n",
        "    fft_abs = np.abs(fft_coefs)\n",
        "\n",
        "    return np.mean(fft_abs), np.std(fft_abs)\n",
        "\n",
        "\n",
        "def extract_wavelet_features(image, mode='haar', level=1):\n",
        "    coeffs2 = pywt.dwt2(rgb2gray(image), mode)\n",
        "    cA, (cH, cV, cD) = coeffs2\n",
        "\n",
        "    return np.mean(cA), np.mean(cH), np.mean(cV), np.mean(cD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    color_stats_features = extract_color_stats(image)\n",
        "    color_histogram_features = extract_color_histogram(image)\n",
        "    haralick_features = extract_haralick_features(image)\n",
        "    lbp_features = extract_lbp_features(image)\n",
        "    fft_features = extract_fft_features(image)\n",
        "    wlt_features = extract_wavelet_features(image)\n",
        "    \n",
        "    features = np.concatenate([\n",
        "        color_stats_features, color_histogram_features, \n",
        "        haralick_features, lbp_features,\n",
        "        fft_features, wlt_features\n",
        "    ])\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_preprocessed = np.array([preprocess_image(image) for image in X_train])\n",
        "X_test_preprocessed = np.array([preprocess_image(image) for image in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_preprocessed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el modelo\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Definir la rejilla de hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'tree_method': ['auto']\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Ejecutar la búsqueda\n",
        "grid_search.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "# Mejores parámetros y mejor puntuación\n",
        "print(\"Mejores parámetros encontrados: \", grid_search.best_params_)\n",
        "print(\"Mejor puntuación: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = grid_search.predict(X_test_preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precisión\n",
        "print(\"Precisión de Gradient Boosting:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Informe de clasificación\n",
        "print(\"\\nInforme de clasificación de Gradient Boosting:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusión\n",
        "print(\"\\nMatriz de confusión de Gradient Boosting:\")\n",
        "cm_rf = confusion_matrix(y_test, y_pred)\n",
        "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)\n",
        "disp_rf.plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
