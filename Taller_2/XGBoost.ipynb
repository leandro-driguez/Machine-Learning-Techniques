{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leandro-driguez/Machine-Learning-Techniques/blob/dev/Taller_2/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SU_lch1DySrJ"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hectormelo/Machine-Learning-Techniques/main/Banner.png\" ><br>\n",
        "# Machine Learning Techniques - MISIS4219\n",
        "\n",
        "Primer semestre - 2024\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pywt\n",
        "import mahotas\n",
        "import requests\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from scipy import stats\n",
        "import mahotas.features\n",
        "from zipfile import ZipFile\n",
        "from joblib import dump, load\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. **Exploración de Datos**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descargando y Descomprimiendo el Conjunto de Datos\n",
        "\n",
        "Antes de que podamos proceder con la carga y el procesamiento de las imágenes, necesitamos obtener el conjunto de datos. Esta sección del cuaderno está dedicada a descargar un archivo ZIP que contiene el conjunto de datos EuroSAT desde una URL de repositorio de GitHub especificada y luego descomprimirlo en un directorio designado. Este paso es crucial para hacer el conjunto de datos accesible para tareas de carga y análisis subsiguientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# URL of the ZIP file in the GitHub repository\n",
        "url = 'https://github.com/hectormelo/Machine-Learning-Techniques/raw/main/Lab_2/EuroSAT3.zip'\n",
        "\n",
        "# Download the ZIP file\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    zip_content = response.content\n",
        "    # Unzip the content in memory\n",
        "    with ZipFile(BytesIO(zip_content)) as zip_file:\n",
        "        # Specify the directory where you want to unzip\n",
        "        zip_file.extractall(\"./EuroSAT3\")\n",
        "    print(\"File successfully unzipped.\")\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargando y Etiquetando Imágenes\n",
        "\n",
        "En esta sección, cargaremos imágenes de diferentes categorías para preparar nuestros datos de entrenamiento. Definimos un conjunto de categorías que corresponden a los nombres de las carpetas donde se almacenan nuestras imágenes. Para cada categoría, cargamos las imágenes, las normalizamos (como se describió en el paso anterior) y asignamos una etiqueta numérica a cada imagen. Esto nos permitirá entrenar un modelo de clasificación de imágenes más adelante en el cuaderno.\n",
        "\n",
        "El proceso es el siguiente:\n",
        "\n",
        "1. Definir las categorías de imágenes que queremos cargar.\n",
        "2. Preparar dos listas: una para almacenar las imágenes y otra para las etiquetas.\n",
        "3. Iterar sobre cada categoría, cargar las imágenes de esa categoría y asignar una etiqueta numérica a cada imagen basada en su categoría.\n",
        "4. Convertir las listas de imágenes y etiquetas en arreglos de NumPy para facilitar su manipulación en pasos posteriores de procesamiento o entrenamiento de modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_images(folder):\n",
        "    \"\"\"\n",
        "    Load and normalize images from a specified folder.\n",
        "\n",
        "    This function iterates over all files in the given folder, assuming they are images.\n",
        "    Each image is converted to an RGB array and normalized to have values between 0 and 1.\n",
        "\n",
        "    Args:\n",
        "        folder (str): The path to the folder containing the images.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of normalized images represented as numpy arrays.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize an empty list to store the normalized images\n",
        "    images = []\n",
        "    \n",
        "    # Iterate over every file in the specified folder\n",
        "    for filename in os.listdir(folder):\n",
        "        # Construct the full path to the file\n",
        "        image_path = os.path.join(folder, filename)\n",
        "        \n",
        "        try:\n",
        "            # Open the image, convert it to RGB to ensure consistency, and convert to numpy array\n",
        "            with Image.open(image_path) as image:\n",
        "                image = np.array(image.convert(\"RGB\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {filename}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Normalize the image data to 0-1 range\n",
        "        normalized_image = image / 255.0\n",
        "        \n",
        "        # Append the normalized image to our list\n",
        "        images.append(normalized_image)\n",
        "    \n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of categories corresponding to the folder names of images\n",
        "categories = ['Forest', 'Industrial', 'PermanentCrop', 'Residential', 'River']\n",
        "base_path = './EuroSAT3/'  # Base path where the category folders are located\n",
        "\n",
        "# Initialize lists to store all images and labels\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "# Iterate over each category to load the images\n",
        "for label, category in enumerate(categories):\n",
        "    # Create the full path to the current category's folder\n",
        "    folder = os.path.join(base_path, category)\n",
        "    # Load all images from the current folder\n",
        "    category_images = load_images(folder)\n",
        "    # Add the loaded images to the general image list\n",
        "    all_images.extend(category_images)\n",
        "    # Assign a corresponding label to each loaded image\n",
        "    all_labels.extend([label] * len(category_images))\n",
        "\n",
        "# Convert the lists of images and labels into NumPy arrays\n",
        "all_images_np = np.array(all_images)\n",
        "all_labels_np = np.array(all_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dividiendo el Conjunto de Datos en Entrenamiento y Pruebas\n",
        "\n",
        "Después de cargar y preprocesar nuestras imágenes y sus correspondientes etiquetas, el siguiente paso crucial es dividir el conjunto de datos en conjuntos de entrenamiento y pruebas. Esto nos permite entrenar nuestro modelo en una porción de los datos (conjunto de entrenamiento) y luego evaluar su rendimiento en datos no vistos (conjunto de pruebas). Utilizando la función train_test_split de sklearn.model_selection, podemos fácilmente particionar el conjunto de datos de acuerdo a esto. Asignaremos el 80% de los datos para el entrenamiento y reservaremos el 20% para las pruebas. Adicionalmente, establecer un random_state asegura que nuestros resultados sean reproducibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_images_np, all_labels_np, test_size=0.2, random_state=42)\n",
        "\n",
        "# Displaying the shapes of the training and testing sets\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizando la Distribución de Clases\n",
        "\n",
        "Después de dividir nuestro conjunto de datos en conjuntos de entrenamiento y prueba, es importante examinar la distribución de clases dentro de nuestro conjunto de entrenamiento. Este paso nos ayuda a comprender el equilibrio (o desequilibrio) entre las diferentes clases, lo cual puede informarnos sobre posibles sesgos en nuestro conjunto de datos y puede influir en nuestra elección de métricas de evaluación de modelos o en la necesidad de técnicas de remuestreo. A continuación, utilizaremos un histograma para visualizar la distribución de clases en nuestros datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the distribution of classes\n",
        "plt.hist(y_train, bins=len(categories), rwidth=0.7)  # Adjust the number of bins to match the number of categories\n",
        "plt.xticks(ticks=range(len(categories)), labels=categories, rotation='vertical')  # Label x-axis with category names\n",
        "plt.title(\"Class Distribution in Training Set\")  # Add a title for clarity\n",
        "plt.xlabel(\"Categories\")  # Label the x-axis\n",
        "plt.ylabel(\"Number of Images\")  # Label the y-axis\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observando Imágenes Aleatorias del Conjunto de Entrenamiento\n",
        "\n",
        "La inspección visual del conjunto de datos es un paso crucial para entender mejor los datos. Al seleccionar y mostrar aleatoriamente imágenes de nuestro conjunto de entrenamiento, podemos obtener una idea de la variedad y calidad de las imágenes de las que nuestro modelo aprenderá. Este paso también nos permite confirmar visualmente si las etiquetas están correctamente asignadas, identificando potencialmente cualquier anomalía o imágenes mal etiquetadas desde el principio. Vamos a mostrar algunas imágenes aleatorias junto con sus etiquetas para ver ejemplos de cada clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))  # Set the figure size for better visibility\n",
        "\n",
        "for i in range(9):  # Display 9 images\n",
        "    plt.subplot(3, 3, i + 1)  # Arrange images in a 3x3 grid\n",
        "    rnd_idx = np.random.randint(0, len(X_train))  # Select a random index\n",
        "    plt.imshow(X_train[rnd_idx])  # Display the image at the random index\n",
        "    plt.title(categories[y_train[rnd_idx]])  # Show the category label as the title\n",
        "    plt.axis('off')  # Hide the axis for a cleaner look\n",
        "\n",
        "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. **Preparación de Datos**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingeniería de Características\n",
        "\n",
        "Este segmento del notebook se enfoca en la extracción de múltiples tipos de características de las imágenes para preparar nuestros datos para el entrenamiento de modelos de aprendizaje automático. A continuación, se detalla el propósito y la relevancia de cada función:\n",
        "\n",
        "- `extract_color_stats(image)`: Esta función extrae estadísticas básicas de color, como la media, desviación estándar, varianza, mediana, moda, valor mínimo y máximo, percentiles (25, 50, 75), sesgo, curtosis y rango de cada canal de color de la imagen. Estas estadísticas proporcionan una visión global de la distribución de intensidades de color en la imagen, lo cual puede ser muy relevante para identificar patrones relacionados con ciertas categorías de imágenes en tareas de clasificación o segmentación. Por ejemplo, en la clasificación de imágenes naturales, ciertas estadísticas de color podrían ayudar a distinguir entre imágenes de bosques y océanos.\n",
        "\n",
        "- `extract_color_histogram(image, bins=32, channel_range=(0, 256))`: Calcula el histograma de color para cada canal de color, lo que proporciona una representación binaria de la distribución de intensidades de color en la imagen. Este tipo de característica es fundamental para entender la distribución general de los colores en la imagen y puede ser crucial para distinguir entre diferentes objetos o escenas que tienden a tener distribuciones de color característicos.\n",
        "\n",
        "- `extract_haralick_features(image)`: Extrae características de textura de Haralick de la imagen convertida a escala de grises. Las características de Haralick, que incluyen aspectos como homogeneidad, contraste, correlación y otros, describen la textura de la imagen, lo cual es especialmente útil para clasificar imágenes basadas en patrones de textura o para identificar áreas dentro de una imagen que comparten propiedades texturales similares.\n",
        "\n",
        "- `extract_lbp_features(image, P=8, R=1, method='uniform')`: Calcula el patrón binario local (LBP) de la imagen en escala de grises, un descriptor de textura que es robusto ante cambios en la iluminación. LBP es ampliamente usado para tareas de reconocimiento de texturas y rostros porque captura la estructura local de una imagen de manera eficaz. La relevancia de extraer LBP radica en su capacidad para representar detalles texturales finos que pueden ser determinantes en la clasificación o análisis de imágenes.\n",
        "\n",
        "- `extract_fft_features(image)`: Aplica la transformada rápida de Fourier (FFT) a la imagen en escala de grises para extraer características basadas en los coeficientes de FFT, como la media y la desviación estándar de las magnitudes de los coeficientes. La FFT permite analizar la imagen en el dominio de la frecuencia, lo cual es útil para identificar patrones periódicos o para distinguir entre texturas basadas en la presencia de frecuencias altas o bajas.\n",
        "\n",
        "- `extract_wavelet_features(image, mode='haar', level=1)`: Utiliza la transformada wavelet para descomponer la imagen en escala de grises en componentes de baja y alta frecuencia. Las características extraídas de los coeficientes de wavelet son útiles para capturar tanto la textura como la estructura a diferentes escalas, lo que puede ser crucial para aplicaciones como la detección de bordes, la clasificación de texturas y la compresión de imágenes.\n",
        "\n",
        "- `preprocess_image(image)`: Esta función integra todas las características extraídas por las funciones anteriores en un solo vector de características para cada imagen. La relevancia de esta función radica en su capacidad para combinar múltiples tipos de información (color, textura, frecuencia) en una representación unificada de la imagen, lo que enriquece el conjunto de datos de entrada para los algoritmos de aprendizaje automático y puede mejorar significativamente el rendimiento de los modelos de clasificación o regresión.\n",
        "\n",
        "La combinación de estas características proporciona una representación detallada y multifacética de las imágenes, aprovechando diferentes aspectos visuales que son esenciales para una amplia variedad de tareas de visión por computadora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_color_stats(image):\n",
        "    \"\"\"\n",
        "    Extract basic color statistics from an image.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of statistical features for each color channel including mean,\n",
        "        standard deviation, variance, median, mode, min, max, 25th percentile,\n",
        "        50th percentile, 75th percentile, skewness, kurtosis, and range.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    for channel in range(image.shape[-1]):\n",
        "        channel_data = image[:, :, channel].ravel()\n",
        "        \n",
        "        mean = np.mean(channel_data)\n",
        "        std = np.std(channel_data)\n",
        "        var = np.var(channel_data)\n",
        "        median = np.median(channel_data)\n",
        "        \n",
        "        mode_result = stats.mode(channel_data)\n",
        "        mode = mode_result.mode\n",
        "        \n",
        "        min_val = np.min(channel_data)\n",
        "        max_val = np.max(channel_data)\n",
        "        percentile_25 = np.percentile(channel_data, 25)\n",
        "        percentile_50 = np.percentile(channel_data, 50)\n",
        "        percentile_75 = np.percentile(channel_data, 75)\n",
        "        skewness = stats.skew(channel_data)\n",
        "        kurtosis = stats.kurtosis(channel_data)\n",
        "        data_range = max_val - min_val\n",
        "        \n",
        "        features.extend([\n",
        "            mean, std, var, median, mode, min_val, max_val,\n",
        "            percentile_25, percentile_50, percentile_75,\n",
        "            skewness, kurtosis, data_range\n",
        "        ])\n",
        "\n",
        "    return features\n",
        "\n",
        "def extract_color_histogram(image, bins=32, channel_range=(0, 256)):\n",
        "    \"\"\"\n",
        "    Extract a color histogram from an image.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "        bins (int): Number of histogram bins.\n",
        "        channel_range (tuple): The range of values for each channel.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The concatenated histograms of all color channels.\n",
        "    \"\"\"\n",
        "    histogram = [np.histogram(image[:, :, i], bins=bins, range=channel_range)[0] for i in range(image.shape[-1])]\n",
        "    return np.concatenate(histogram)\n",
        "\n",
        "def extract_haralick_features(image):\n",
        "    \"\"\"\n",
        "    Extract Haralick texture features from an image.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The mean Haralick features across the four directions.\n",
        "    \"\"\"\n",
        "    gray_image = rgb2gray(image)\n",
        "    gray_image = (gray_image * 255).astype('uint8')\n",
        "\n",
        "    features = mahotas.features.haralick(gray_image).mean(axis=0)\n",
        "    \n",
        "    return features\n",
        "\n",
        "def extract_lbp_features(image, P=8, R=1, method='uniform'):\n",
        "    \"\"\"\n",
        "    Extract Local Binary Pattern (LBP) features from an image.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "        P (int): Number of circularly symmetric neighbour set points.\n",
        "        R (float): Radius of circle.\n",
        "        method (str): Method to extract LBP features.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The histogram of LBP features.\n",
        "    \"\"\"\n",
        "    image_gray = rgb2gray(image)\n",
        "    lbp = local_binary_pattern(image_gray, P=P, R=R, method=method)\n",
        "\n",
        "    lbp_hist, _ = np.histogram(lbp, density=True, bins=np.arange(0, P + 3), range=(0, P + 2))\n",
        "    return lbp_hist\n",
        "\n",
        "def extract_fft_features(image):\n",
        "    \"\"\"\n",
        "    Extract features from the magnitude spectrum of the image's FFT.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Mean and standard deviation of the FFT magnitude spectrum.\n",
        "    \"\"\"\n",
        "    fft_coefs = np.fft.fft2(rgb2gray(image))\n",
        "    fft_abs = np.abs(fft_coefs)\n",
        "\n",
        "    return np.mean(fft_abs), np.std(fft_abs)\n",
        "\n",
        "def extract_wavelet_features(image, mode='haar', level=1):\n",
        "    \"\"\"\n",
        "    Extract features using wavelet decomposition from an image.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "        mode (str): Type of wavelet to use.\n",
        "        level (int): The level of wavelet decomposition.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Mean values of the approximation and detail coefficients.\n",
        "    \"\"\"\n",
        "    coeffs2 = pywt.dwt2(rgb2gray(image), mode)\n",
        "    cA, (cH, cV, cD) = coeffs2\n",
        "\n",
        "    return np.mean(cA), np.mean(cH), np.mean(cV), np.mean(cD)\n",
        "\n",
        "def preprocess_image(image):\n",
        "    \"\"\"\n",
        "    Preprocess an image by extracting comprehensive features.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The image array in RGB format.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: A concatenated feature vector consisting of color statistics,\n",
        "        color histogram, Haralick, LBP, FFT, and wavelet features.\n",
        "    \"\"\"\n",
        "    color_stats_features = extract_color_stats(image)\n",
        "    color_histogram_features = extract_color_histogram(image)\n",
        "    haralick_features = extract_haralick_features(image)\n",
        "    lbp_features = extract_lbp_features(image)\n",
        "    fft_features = extract_fft_features(image)\n",
        "    wlt_features = extract_wavelet_features(image)\n",
        "    \n",
        "    features = np.concatenate([\n",
        "        color_stats_features, color_histogram_features, \n",
        "        haralick_features, lbp_features,\n",
        "        fft_features, wlt_features\n",
        "    ])\n",
        "    return features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocesamiento de los datos\n",
        "\n",
        "Aplicación de las técnicas antes mencionadas al conjunto `Train` y `Test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_preprocessed = np.array([preprocess_image(image) for image in X_train])\n",
        "X_test_preprocessed = np.array([preprocess_image(image) for image in X_test])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_clf = xgb.XGBClassifier(\n",
        "    use_label_encoder=False, \n",
        "    eval_metric='mlogloss',\n",
        "    tree_method='hist',\n",
        "    # device='cuda',\n",
        "    max_bin=1024**3,\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150],\n",
        "    'learning_rate': [0.1, 0.2],\n",
        "    'max_depth': [4, 5, 6],\n",
        "    'colsample_bytree': [0.7, 1.0],\n",
        "    'subsample': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "\n",
        "# Ajusta GridSearchCV para buscar otros parámetros excepto tree_method y max_bin\n",
        "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Ejecutar la búsqueda\n",
        "# Asegúrate de tener un conjunto de validación o usar la validación cruzada para determinar la parada temprana\n",
        "grid_search.fit(X_train_preprocessed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the best parameters found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# save best model\n",
        "dump(best_model, 'best_xgboost.joblib')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. **Predicciones**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = load('best_xgboost.joblib')\n",
        "\n",
        "# Predicting on the test set using the best model found by GridSearchCV\n",
        "y_pred = best_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculating and displaying the model's accuracy\n",
        "print(\"Accuracy of XGBoost:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Generating and displaying the classification report, which includes various evaluation metrics\n",
        "print(\"\\nClassification Report for XGBoost:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Generating and visualizing the confusion matrix to assess the model's performance in detail\n",
        "print(\"\\nConfusion Matrix for XGBoost:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
